### 1. 첫 셋팅

```
// 방화벽 없애기
systemctl stop firewalld
systemctl disable firewalld
//ip 바꾸기
cd /etc/sysconfig/network-scripts/
ls
vi ifcfg-ens32
	BOOTPROTO="none"
	IPADDR="192.168.111.120"
	NETMASK="255.255.255.0"
	GATEWAY="192.168.111.2"
	DNS1="192.168.111.2"
systemctl restart network
ifconfig
// host 이름 바꾸고 호스트 정보 입력
hostnamectl set-hostname mainserver
vi /etc/hosts
	192.168.111.120 mainserver
	192.168.111.121 secondserver
	192.168.111.122 dataserver

// 재시작하면 호스트명이 바뀐다
reboot		

```



### 2. jdk 설치

```
www.orcle.com에 가서 download linux jdk x64

cd 다운로드
tar xvf jdk...tar.gz
mv jdk1.8.0_261 jdk1.8.0
cp -r jdk1.8.0 /usr/local
cd /usr/bin
ln -s /usr/local/jdk1.8.0/bin/java java
ls -l java
```

### 3. hadoop 설치

```
cd 다운로드
wget https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
tar xvf hadoop-1.2.1.tar.gz
cp -r hadoop-1.2.1 /usr/local
vi /etc/profile 

	 52	JAVA_HOME=/usr/local/jdk1.8.0
     53 CLASSPATH=/usr/local/jdk1.8.0/lib
     54 HADOOP_HOME=/usr/local/hadoop-1.2.1
     55 export JAVA_HOME CLASSPATH HADOOP_HOME
     56 PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:.:$PATH
     
     
source /etc/profile					// 변경된 profile을 시스템에 적용
```



### 4. SSH 설정

```
// 보안설정
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa			//아무데서나 실행가능
cd .ssh
cat id_dsa.pub >> authorized_keys

// 공개키 복사
ssh-copy-id -i /root/.ssh/id_dsa.pub root@secondserver
yes
ssh-copy-id -i /root/.ssh/id_dsa.pub root@dataserver
yes
ssh-copy-id -i /root/.ssh/id_dsa.pub root@mainserver
yes

// 접속시도해보기
ssh secondserver
```



### 5. Configuration

- core-site.xml

```
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://mainserver:9000</value>			// 현 hostname으로 변경
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop-1.2.1/tmp</value>
</property>
</configuration>
```

- hdfs-site.xml

```
<configuration>
<property>
<name>dfs.replication</name>
<value>2</value> 								// 복제 2개
</property>
<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>
<property>
<name>dfs.name.dir</name>
<value>/usr/local/hadoop-1.2.1/name</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/usr/local/hadoop-1.2.1/data</value>
</property>
</configuration>
```

* mapred-site.xml

```
<configuration>
<property>
<name>mapred.job.tracker</name>
<value>mainserver:9001</value>				// 현 hostname으로 변경
</property>
</configuration>
```



* hadoop-env.sh

```
9 export JAVA_HOME=/usr/local/jdk1.8.0		(변경)

10 export HADOOP_HOME_WARN_SUPPRESS="TRUE"	(추가)
-> 하둡 재구동시
Warning: $HADOOP_HOME is deprecated. 라는 에러가 뜨는 것을 방지
```

* masters

```
secondserver
```

* slaves

```
secondserver
dataserver
```



### 6. 네임노드에서 데이터노드로 java & hadoop 옮기고 실행

```
scp hadoop-1.2.1.tar.gz root@secondserver:/root
scp hadoop-1.2.1.tar.gz root@dataserver:/root
ssh root@secondserver "tar xvf hadoop-1.2.1.tar.gz; rm hadoop-1.2.1.tar.gz"
ssh root@secondserver "cp -r hadoop-1.2.1 /usr/local"
ssh root@dataserver "tar xvf hadoop-1.2.1.tar.gz; rm hadoop-1.2.1.tar.gz"
ssh root@dataserver "cp -r hadoop-1.2.1 /usr/local"
cd 다운로드
ls
scp jdk-8u261-linux-x64.tar.gz root@secondserver:/root
scp jdk-8u261-linux-x64.tar.gz root@dataserver:/root
ssh root@secondserver "tar xvf jdk-8u261-linux-x64.tar.gz; rm jdk-8u261-linux-x64.tar.gz"
ssh root@dataserver "tar xvf jdk-8u261-linux-x64.tar.gz; rm jdk-8u261-linux-x64.tar.gz"
ssh root@secondserver "mv jdk1.8.0_261 jdk1.8.0"
ssh root@dataserver "mv jdk1.8.0_261 jdk1.8.0"
ssh root@secondserver "cp -r jdk1.8.0 /usr/local"
ssh root@dataserver "cp -r jdk1.8.0 /usr/local"
```

### 7. Hadoop 실행

````
# 실행
hadoop namenode -format
start-all.sh
jps
# 종료
stop-all.sh
````



**firefox에서 http://서버이름:50070 로 확인**

