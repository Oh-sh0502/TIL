# Hadoop2 - YARN

### 등장 배경

 하둡의 등장으로 빅데이터를 처리할 수 있게 되었고, 하둡을 중심으로 한 에코시스템이 활성화되면서 다양한 방법으로 데이터를 저장하고 분석할 수 있게 되었다.  
 그러나 이는 하둡 커뮤니티와 운영자에게 **리소스관리**의 과제를 남겼다.

- 하나의 클러스터에서 다양한 하둡 에코시스템이 적절하게 시스템자원을 할당받고, 할당된 자원이 모니터링되고 해제되어야하는데, **기존의 하둡이 체계가 잡히지 않음**.

- **단일 고장점(SPOF: Single Point Of Failure)인 네임노드의 이중화문제**
  : 데이터노드의 블록들이 하나의 네임스페이스만 사용하는 데에 따른 단점이 제시되고 개선 요청이 잇따름

  > **"네임노드 망가지면 데이터노드고 뭐고 못 써버림"**

- **하둡1은 맵리듀스 API로 개발된 애플리케이션만 실행할 수 있음**

<br>

<br>

**이러한 이유들로 3년의 준비 끝에 2013년 10월, 하둡 2.2.0 버전(하둡2) 등장**

---

### 하둡2의 주요기능

- YARN
- 네임노드 고가용성
- HDFS 페더레이션
- HDFS 스냅샷
- NFSv3 파일 시스템 지원
- 성능 개선

<br>

## YARN(Yet Another Resource Negotiator)

> 하둡2의 가장 큰 변화. 맵리듀스 외에 다른 종류의 애플리케이션도 실행가능한 리소스 협상가.

<br>

### YARN 등장배경(이유)

- **맵리듀스 단일 고장점(SPOF: Single Point Of Failure)**  
   잡트래커는 모든 맵리듀스의 잡의 실행 요청을 받고, 전체 잡의 스케줄링 관리와 리소스 관리를 담당. 따라서, **맵리듀스 잡을 실행하려면 반드시 잡트래커가 실행 중이어야 한다**. 태스크트래커가 실행 중이어도 **잡트래커가 돌아가고 있지 않으면 맵리듀스 잡 실행불가**.
- **잡트래커 메모리 이슈**  
   잡트래커는 메모리에 전체 잡의 실행 정보를 담고 있고, 이를 맵리듀스 잡 관리에 활용한다. 이 메모리에 많은 정보를 유지하고 있다보니 잡트래커도 많은 메모리가 필요하다. (실제로 하둡 클러스터 운영에서 잡트래커에게 힙 메모리를 여유 있게 할당함)  
   이러한 **잡트래커가 메모리가 부족하다면 잡의 상태를 모니터링할 수 없고, 새로운 잡의 실행을 요청할 수도 없다.**
- **맵리듀스 리소스 관리 방식**  
   맵리듀스는 **슬롯**이라는 개념으로 태스크 개수를 관리했다. 슬롯은 맵 태스크 실행을 위한 `맵 슬롯`과 리듀스 태스크 실행을 위한 `리듀스 슬롯`으로 구분되는데, **만약 실행중인 잡이 맵 슬롯만 사용하고 있거나 혹은 리듀스 슬롯만 사용하고 있으면 다른 슬롯이 잉여 자원이 되어 리소스 낭비가 발생**.
- **클러스터 확장성**  
   하둡1의 **단일 클러스터는 최대 4000대**, **최대 동시 실행 태스크는 40000개**가 한계이다. 또한 맵리듀스가 맵과 리듀스 외에 다른 알고리즘을 지원하는데에 한계가 있다.
- **버전 통일성**  
   하둡1은 맵리듀스 잡을 요청하는 **클라이언트와 맵리듀스 클러스터의 버전이 반드시 동일**해야 한다.

<br>

### YARN의 특징

<img src="https://user-images.githubusercontent.com/71415474/114820144-847af400-9df9-11eb-96f7-ff6022b1505c.PNG" alt="hadoop_22" style="zoom:80%;" />

<br>

YARN은 **두 가지 설계목표**가 있었다.

1. **잡트래커 주요 기능 추상화**  
   잡트래커의 핵심기능인 **클러스터 자원 관리와, 애플리케이션 라이프 사이클 관리를 분리**하고 새로운 추상화 레이어를 만들었다.
2. **다양한 데이터 처리 애플리케이션 수용**  
    맵리듀스 API로 구현된 프로그램만 실행하는 구조에서 **맵리듀스를 얀에서 실행되는 애플리케이션 중 하나로 인식하는 구조**로 변경

<br>

이를 바탕으로한 YARN의 특징은 다음과 같다.

- **확장성**  
  수용 가능한 **단일 클러스터 규모를 4000에서 10000 노드까지 확대**. 이로 인해, **처리 가능한 데이터 처리 작업 개수도 증가**

  > 2013년 야후는 얀 클러스터로 하루에 40만개의 잡을 수행함

- **클러스터 활용 개선**  
   이제 자원관리는 **얀의 새 컴포넌트인 리소스 매니저(ResourceManager)**가 담당한다. **리소스 매니저는** 기존 맵리듀스가 가진 맵 슬롯 & 리듀스 슬롯 개념에서  **CPU, 메모리, 디스크, 네트워크 등을 실제 가용한 단위로 자원을 관리하고, 얀에서 실행되는 애플리케이션에 자원을 배분함.**

- **워크로드 확장**  
  하이브, 피그 등 맵리듀스 기반의 애플리케이션 뿐만아니라 인터랙티브 질의 ,실시간 처리, 그래프 알고리즘 등 **다양한 형태의 워크로드 확장 가능**

- **맵리듀스 호환성**  
  기존 맵리듀스 프로그램 코드도 얀에서 실행 가능

<br>

### 얀 아키텍처

![hadoop_23](https://user-images.githubusercontent.com/71415474/114822043-72e71b80-9dfc-11eb-956d-16dac751564e.PNG)

얀 아키텍처는 크게 **네 종류의 컴포넌트**로 구성

1. **리소스 매니저**
   - 전체 클러스터에서 가용한 **모든 시스템 자원을 관리**
   - 얀 클러스터에서 실행되는 **애플리케이션이 리소스를 요청하면 이를 적절하게 분배하고, 리소스 사용 상태 모니터링**
   - **얀의 마스터 서버**에 해당
2. **노드 매니저(NodeManager)**
   - **맵리듀스의 태스크트래커** 기능
   - 태스크트래커와 마찬가지로 **각 슬레이브 서버에서 하나의 데몬이 실행**
   - **컨테이너(Container)를 실행**하고, **컨테이너의 라이프 사이클을 모니터링**
3. **컨테이너**
   - **노드매니저가 실행되는 서버의 시스템 자원(CPU, 메모리, 디스크, 네트워크 등)을 표현.**
   - **리소스 매니저의 요청에 의해 실행**되며, **하나의 서버에 시스템 자원현황에 따라 여러 개의 컨테이너가 실행될 수 있음**
   - **노드매니저는 컨테이너 단위로 애플리케이션을 실행**하고, 각 상태를 스케쥴링함.
4. **애플리케이션마스터(ApplicationMaster)**
   - **애플리케이션을 관리하는 마스터 서버**
   - 클라이언트가 얀에 애플리케이션 실행을 요청하면, **얀은 하나의 애플리케이션에 하나의 애플리케이션마스터를 할당.**
   - **애플리케이션에 필요한 리소스를 스케줄링**
   - **노드매니저에게 애플리케이션이 필요한 컨테이너를 실행할 것을 요청**

---

### Reference

- 참고 문헌: 시작하세요! 하둡 프로그래밍(Beginning Hadoop: Learn from basic to practical techniques)l

  